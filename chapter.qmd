---
title: "Using R with RStudio & Tidyverse"
author: "Arend M Kuyper"

# format:
#   html:
#     toc: true
#     toc-location: left
#     embed-resources: true
#     link-external-newwindow: true
#     code-fold: true
#     fig-dpi: 300

format: docx

crossref: 
  subref-labels: alpha A
  
nocite: |
  @r-project, @tigris, @tidycensus, @pkg-tidyverse
  
execute:
  warning: false
  
bibliography: 'https://api.citedrive.com/bib/5d5433a7-708d-44c1-92c7-9561963807cd/references.bib?x=eyJpZCI6ICI1ZDU0MzNhNy03MDhkLTQ0YzEtOTJjNy05NTYxOTYzODA3Y2QiLCAidXNlciI6ICIxMjc3OCIsICJzaWduYXR1cmUiOiAiNDYzYWRlM2ExZGFlMmE2ZDQ1MDlkYmY1ZjY5OTZhOWRhMjEwNjQyYmViYjY5NWYwNzM0YzE3NzI3YmM0NTkxOSJ9'
csl: 'https://raw.githubusercontent.com/citation-style-language/styles/master/apa-annotated-bibliography.csl'

from: markdown+emoji 
---

## Introduction

The purpose of this chapter is to provide you with an introduction to the free statistical software R along with RStudio and the tidyverse packages. While R can have a steep learning curve and be intimidating to new users, especially those new to coding, RStudio and the tidyverse packages make R much more accessible. Beyond improving accessibility, these tools are intentionally designed to make users more productive with R and improve the reproducibility of their work. Not surprisingly, this chapter is focused on the "how to" of foundational data work. We will highlight and demonstrate essential best practices for using R with RStudio and the tidyverse for quantitative research. However, don't let this chapter's focus on first steps of data work deceive, R and its ecosystem of extension packages allow for the implementation of all the statistical techniques found in this book and much, much more.

## R

R is a powerful and highly flexible statistical analysis tool. It provides a wide array of statistical techniques and methods while also providing highly developed graphics capabilities. R's ability to create publication-quality graphics has been, and continues to be, one of its greatest strengths. It does all of this free of charge with a dedicated community of developers.

The R project is a GNU project, which means it is a free software, mass collaboration project. Knowing that R is open source and is actively developed and maintained through mass collaboration provides important context for users concerning its basic structure and potential resources. R can be considered as being made up of two 2 parts:

1.  the base R system that is downloaded from the Comprehensive R Archive Network, also known as CRAN, and
2.  a large ecosystem of extension packages, sometimes called libraries.

The base R system is actively maintained and updated for various operating systems by the R Core Team. Typically there is 1 major update along with 2 minor updates per year. Having an active release schedule like this is critical for the success of open source software. This ensures up-to-date compatibility with operating systems and signals to users that it won't be abandoned.

While the base R system adequately covers most statistical needs and functionality, it is arguably the large ecosystem of extension packages that has contributed to R's growth. Packages can provide implementations of methodologies not currently in base R, improve the usability of R, or provide tools that allow you to do non-statistical tasks (e.g. sending emails or building websites). In particular, the tidyverse packages have been very influential and have made working with R significantly more accessible. We will take a closer look at the tidyverse packages later in this chapter.

As of this writing, the CRAN package repository features over 21,000 contributed packages. The CRAN repository checks all packages for compatibility and expects the packages to be maintained, which means users can expect packages from the CRAN repository to work with R. This work is done by a network of volunteers, called the CRAN team, and it is a testament to the size and dedication of the R community that this is possible. Though CRAN is the primary R package repository, users can find packages through:

-   CRAN-like repositories such as BioConductor and R-forge;
-   GitHub and BitBucket;
-   Personal websites.

While packages outside of the CRAN repository aren't vetted by the CRAN team for compatibility, they can be very useful. They may implement cutting edge statistical techniques or provide tools for more bespoke analyses. Going through the CRAN submission process can be daunting, time intensive, and restrictive so it is not uncommon to find very useful packages not hosted on CRAN.

### Using R

A common roadblock for many new R users is that it requires the users to write code or commands. This can be a significant hurdle for many, but there are several free software options that make working with R much more user-friendly. The most popular being RStudio, which we will discuss in more detail later on in the chapter. The need to write code or commands isn't removed, but it is made much more intuitive and accessing help is made easier. Using R and having to write code is a net positive for increasing the reproducibility of research, at least for computational and analysis work.

The value of learning to write R code is significantly enhanced by following best practices for coding and setting up workflows. When users are first learning it can seem unnecessary to follow such advice, but it is important to avoid developing bad and inefficient habits. RStudio and the tidyverse are specifically designed to guide users to follow and implement best practices. We will be highlighting and demonstrating some of these best practices in the following sections, but readers wanting more guidance should see the **Suggested further readings**.

## RStudio

RStudio is an integrated development environment (IDE) designed to make working with R more accessible and productive. While R comes with its own graphical user interface (GUI), it simply was not designed with a wide range of users in mind. So, it is common that R be paired with some other open source software such as RStudio, R Commander, Deducer, jupyter notebooks, vscode, or positron. RStudio is by far the most widely used and known. A significant portion of R's growth in usage can be reasonably attributed to RStudio. It has become synonymous with R. RStudio can be downloaded for free from <https://posit.co/download/rstudio-desktop/>.

When RStudio is first opened, there are 4 panes as seen in @fig-rstudio-layout. Sometimes the source pane is missing, but that is easily remedied by opening a new R script (.R file): **File \> New File \> R Script**.

1.  The **Source pane** is where you can edit and save R scripts, which are essentially text files containing R code. This is where most of the data analysis work happens and should be documented.

2.  The **Console pane** is used to write short interactive R commands.

3.  The **Environment pane** displays temporary R objects as created during that R session. It also contains the useful history tab.

4.  The **Output pane** displays the plots, tables, or HTML outputs of executed code along with saved files. This pane also includes the packages and help tabs which are especially useful since the first is for managing and installing packages and the second is setup to help access documentation.

![RStudio's basic layout](image/rstudio-layout.png){#fig-rstudio-layout fig-alt="RStudio's basic layout consisting of 4 panes: source, environment, console, & files" fig-align="center"}

### Prepare for success

Before starting to work with data, there are 2 best practices that should be discussed:

1.  Adjusting a few RStudio default options to improve long-term reproducibility of your data analysis work.

2.  Use RStudio projects to improve organization and collaboration, which includes with your future self.

By default, workspaces will load everything that you had been working on previously, from .Rdata files. While this might sound harmless or even desirable, it actually creates a workflow that could easily lead to ghost or zombie objects. That is, objects that are not reproducible because we may have ran code out of order ot altered code and forgot to re-run it to update things. By being automatically saved we might not catch this error until it is way too late. So, to develop R scripts that are complete and self-contained records of the data work we need to make a few adjustments. In RStudio, set this via **Tools \> Global Options**, uncheck "Restore .RData into Workspace at Startup" and choose **Never** on the "Save workspace to .RData on exit" as seen in @fig-rstudio-wrk-options.

![RStudio's Globol Options: Workspace Best Practice](image/rstudio-workspace-options.png){#fig-rstudio-wrk-options fig-alt="Adjusting RStudio's global options for workspaces to not restore at startup and never save on exit." fig-align="center" width="696"}

The larger and more sprawling research and analysis work becomes, the more important it is to be organized. Meaning, it is important to have a "home base" of operations where everything for your data analysis or research work is self contained. By using RStudio projects it becomes straightforward to organize your work. You can think of an RStudio project as being a home directory/folder that will ultimately contain everything for your data analysis project. RStudio projects provide a solid workflow that will serve you well in the future:

-   When starting a data analysis project (or any work in R), create a new RStudio project,
-   Keep all data files there; organized in a data sud-directory.
-   Keep all scripts there; edit them, run them in bits or as a whole. Naming them sequentially (e.g. 0-loading-data.R, 1-inspecting-data.R, etc) is recommended.
-   Save your outputs (plots and cleaned data) there. Organized into appropriate sub-directories or folders

By using an RStudio project, everything you need is in one place, and cleanly separated from all the other projects that you are working on. The also make collaboration easier. The folders could be maintained on a shared drive so or version control software like git can be integrated into the projects. If used appropriately, then we should be able to simply zip/compress the RStudio project folder and share it with anyone else. 

To create a new project in the RStudio, use the **File \> New Project**. In the New Project wizard that pops up, select **New Directory**, then **New Project**. Pick a name for the project, for example "cwift-examples" (name of project for examples provided later in this chapter), and then click the **Create Project** button. This will launch a new RStudio Project inside a new folder called "cwift-examples". The name of the project should appear in the top-right hand corner of Rstudio as seen in @fig-cwift-examples-project. A good piece of advice, if it says "Project: (None)", then don't do any work. Set up a miscellaneous project for scratch work and musings. Always be working in a project.

## Tidyverse

> The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. [@Tidyverse-2024-08-15]

This generally makes the tidyverse syntax easier to read and implement, especially for those new to coding and data analytic work. It makes chaining together data processing steps intuitive and easier than in other programming languages. The core tidyverse packages are those most likely needed for everyday data analyses and are all loaded by the meta `{tidyverse}` package. As of `{tidyverse}` 2.0, the following 9 packages are included in the core tidyverse:

-   `{ggplot2}` is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.

-   `{dplyr}` provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.

-   `{tidyr}` provides a set of functions that help you get to tidy data. Tidy data is data with a consistent form: in brief, every variable goes in a column, and every column is a variable.

-   `{readr}` provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf).

-   `{purrr}` enhances R’s functional programming toolkit by providing a complete and consistent set of tools for working with functions and vectors.

-   `{tibble}` is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not.

-   `{stringr}` provides a cohesive set of functions designed to make working with strings as easy as possible.

-   `{forcats}` provides a suite of useful tools that solve common problems with factors (categorical variables).

-   `{lubridate}` provides a set of tools to make it easier to work with dates and times in R, which can be difficult and inconsistent in base R.

To install the tidyverse packages, go to **output pane \> Packages \> Install**. The package install wizard will pop-up and enter "tidyverse" as the name of the page to download. Ensure that "Install dependencies" is checked, then click **Install**. Alternatively, we could have ran `install.packages("tidyverse")` in the console. Not only will we get the 9 core tidyverse packages, but all of the packages they are dependent on will also be installed. 

Beyond the core tidyverse packages, we will be using several additional packages as we work through a few data explorations in the next section. Packages such as `tidycenus` (tools to obtain census data in tidy format), `{janitor}` (tools for cleaning data), `{sf}` (tools for making maps), and `{skimr}` (tools to quickly skim data). Packages like `{tidycensus}` and `{janitor}` are often described as tidyverse-adjacent because they are built to work with the tidyverse and therefore adhere to the shared design philosophies. 

The tidyverse's influence on the R package ecosystem has been transformative. It has laid a foundation upon which developers in a large open-source community can collaborate and build tools that are much more accessible (e.g. tidyverse-adjacent packages). Developers can build their packages to leverage all existing tools and benefits of the tidyverse ecosystem which helps to alleviate dependency issues and compatibility with current and future features. Ultimately, this means users will be able to pick up new packages much more efficiently because of the shared design philosophies and standards. 

## Data Exploration Examples

Time to see R with RStudio and tidyverse in action. We will providing highlights from a short exploration of educational data that has a spatial or geographical component. The objective is to demonstrate the usefulness and flexibility of using these tools for data work. So we will be sharing and discussing the outputs while refraining from going through code which would be of little value unless you have started to learn R and the tidyverse. For those interested in seeing more details and/or wanting to work through the code directly, then visit the GitHub repository (<https://github.com/akuyper/cwift-examples>) for this data exploration. Regardless, we encourage readers to explore the GitHub repository because it is a demonstration of best practices when working with data. 

Before taking a closer look at the data and exploration work, let's peak at the layout of the project and discuss what we see in @fig-cwift-examples-project:

- It is clear we are working in a project because the project name, "cwift-examples", is visible in the top-right corner. 
- The R script in the Source Pane is well organized. It has comments to help guide humans through the code, it starts with a preamble that loads the necessary packages and data, and we see indentation and white/negative spaces being used to make the code more readable --- also makes it easier to debug. It follows The tidyverse style guide [@tdyverse-sytle-guide]. As a best practice, a consistent style guide should always be used with a data project.
- The Environment Pane contains items created by running the code. 
- The Console Pane has remnants of code being ran (it is pushed from the source and executed/ran in the console). 
- The Output Pane is displaying the Files tab and we can see the project well organized with appropriate sub-directories and file names.

![Layout overview for the cwift-examples project](image/project-overview.png){#fig-cwift-examples-project fig-alt="Layout overview for the cwift-examples project" fig-align="center"}

A closer look at the R Script in @fig-cwift-examples-project reveals that `|>` operator is being used repeatedly. This is the pipe operator and the tidyverse is designed to use it to build data pipelines (i.e. take a dataset and perform a series of actions on it). Code writing and readability is generally easier when using the pipe. 

Consider @fig-piping, it provides a simple example demonstrating the difference between using the pipe or not. We have a dataset, called `my_data`, and we want to sequentially apply actions `h()`, `g()`, and `f()`, That is, we want to take `my_data`, *and then* apply `h()`, *and then* apply `g()`, *and then* finally apply `f()`. Replacing the *and then*'s with `|>` naturally leads to the piping example in @fig-piping. The non-piping requires that we work from the inside out which is not nearly as smooth or natural. 

Be aware that there is another pipe operator, `%>%`. It is commonly used since comes with the tidyverse and pre-dated `|>`. The two generally work the same, so it is best to pick one and stick with it as a matter practice. `|>` is commonly called the native pipe because it was added to base R. Meaning, `|>` is always available in R. We don't have to load the `tidyverse` or more specifically the `maggrittr` package to have a pipe operator.

![Non-piping or piping in R](image/piping.png){#fig-piping fig-alt="Non-piping vs piping" fig-align="center"}

### The data

We will be using the 2021 American Community Survey Comparable Wage Index for Teachers (ACS-CWIFT or CWIFT) data from the Education Demographic and Geographic Estimates (EDGE) Program [@edge-data]. 

> The Comparable Wage Index (CWI) is an index that was initially created by the National Center for
Education Statistics (NCES) to facilitate comparison of educational expenditures across locales
(principally school districts, or local educational agencies—LEAs) or states (state educational agencies—
SEAs). The CWI is a measure of the systematic, regional variations in the wages and salaries of college
graduates who are not PK-12 educators as determined by reported occupational category. It can be
used by researchers to adjust district-level finance data at different levels in order to make better
comparisons across geographic areas. [@edge-cwift-tech-report]

The basic idea behind any CWI is that workers will demand higher wages in areas with high costs of living or where commonly sought after local amenities such as a good climate, low crime rates, and museums are lacking. This means, it should be possible to measure geographic variation in the cost of hiring teachers (PK-12 educators) by observing systematic, regional variations in the wages of comparable workers who are not PK-12 educators. Put another way, say accountants, nurses, data analyst, and software engineering all earn 20% more than the national average for their professions in San Francisco, then it is reasonable to expect that the cost of hiring teachers in San Francisco would also be 20% more than the national average for teachers.

A core motivation for the developing CWIFT estimates is to enable more meaningful comparisons concerning teacher expenditures across school districts. That is, we can use the CWIFT to normalize dollar amounts and make them comparable by dividing the dollar amounts by the district-level CWIFT, which are already normalized to the national average wage. 

Suppose we wanted to make comparisons between salary expenditure data from the Elementary and Secondary Information (ELSI) system for the 2019-20 school year [@usdoe2023ccd]. The total current expenditures on salary per pupil for the Los Angeles Unified School District was \$6,137 and was \$5,433 in Palm Beach County School District (Florida). So it appears Los Angeles Unified is spending \$704 more on salary per pupil. This is obviously misleading comparison since the cost of living is very different betwen these two areas. When adjust for their difference in purchasing power using their 2021 CWIFT estimates, Los Angeles Unified is \$5,369 (\$6,137 / 1.143) and Palm Beach County is \$5,601 (\$5,433 / 0.970). The story has flipped and Palm Beach effectively spent \$232 more per pupil than Los Angeles Unified.

Another useful application of the CWIFT is to adjust state aid to a school district based on differences in wages due to geographical variations. Suppose a state, say South Dakota, is considering a program intended to provide an additional \$100 per pupil in wages. An extra \$100 per pupil does not go as far in different locations around the state (i.e. geographic variations in the cost of education). The 2021 CWIFT for Harrisburg, SD is 0.895, or about 10% lower than the national average; the 2021 CWIFT for Pierre, SD is 0.739, or approximately 26% lower than the national average. To receive the same increase in purchasing power as a \$100 increase in the Pierre School District, the Harrisburg School District would need to receive \$121.11 (\$100*(0.895 / 0.739)).

There are 3 data files containing CWIFT estimates at the state, county, and LEA (Local Education Agency; typically school districts). The general structure of each data file:

- CWIFT estimate
- Standard error of the CWIFT estimate
- Name of geographic unit (State, county, or LEA)
- Unique identifier (State/County FIPS code or LEA ID)

@fig-state-data provides a quick peak at the strucure of the state level data file. 

![The first 15 observations in the state CWIFT dataset](image/state-data.png){#fig-state-data fig-alt="The first 15 observations in the state CWIFT dataset" fig-align="center"}

These are simple data files, but what makes them interesting is their geographic/spatial components and our ability to quickly join mapping data and other geographic/spatial measurements using the `tigris` and `tidycensus` packages. `tigris` provides access to mapping data needed to build appropriate visualizations and `tidycensus` allows us to access interesting variables from the US Census such as median household income estimates. 

### Exploring State CWIFTs 

The exploration with state CWIFT estimates. We can obtain a summary report using the `skimr` package. In one short call we have a report that breaks down our data and provides appropriate summary statistics for the variable types, as seen in @fig-skimr-state-output. We are particularly interested in the `st_cwiftest` row since that is our variable of interest. The mean (0.952) is larger than median (0.912), hinting at some asymmetry in the distribution. The values range from a 0.809 to 1.27, while 75% of the values are less than or equal to 1. Numerical summaries are useful and informative, but visual inspections should always be done.

![Summary report on state CWIFT dataset produced by `skimr` package](image/skimr-state-output.png){#fig-skimr-state-output fig-alt="Summary report on state CWIFT dataset produced by `skimr` package" fig-align="center"}

@fig-state-plot-collection provides a collection of plots used to visually inspect state CWIFT estimates. The images also display different levels of graphical adornment. @fig-state-plot-collection(B) and @fig-state-plot-collection(C) are basic run the code with default settings, while @fig-state-plot-collection(A) and @fig-state-plot-collection(D) use a different theme and clean titles up a little. This collection of plots are inline with the numerical summary, but @fig-state-plot-collection(A) also provides an ordering of states and whether their CWIFT are statistically different from one another at a 0.05 level. As we would expect, high cost of living states like California and New are towards the top of the list (larger CWIFT values) and low cost of living states like West Virginia and South Dakota are towards the bottom (smaller CWIFT values). Unless you have a very good sense of US geography it would be difficult to detect spatial relationships between the state CWIFTs. 

![Collection of various plots for examining state CWIFT estimates: (A) Approximate 95% confidence intervals for state CWIFT estimates (B) Density plot for state CWIFT estimates (C) Boxplot for state CWIFT estimates, (D) A density dot plot for state CWIFT estimates](plots/state_plot_collection.png){#fig-state-plot-collection fig-alt="Collection of various plots that can be created when examining state CWIFT estimates" fig-align="center"}

Accordingly, @fig-state-heatmaps displays two choropleth maps to explore the spatial relationships between state CWIFTs. The maps appear to show that coastal states have higher values for CWIFTs, while the interior states have lower values. To build these, we used the `tigris` package to obtain mapping data, performed a little data wrangling to join the data sets, and then made the maps. Each map was completed in about 15-20 total lines of code, which is quite remarkable. It is a testament to the tidyverse tools and the general R package ecosystem.

![Two types of choropleth/heat maps for examining state CWIFT estimates: (A) US state map with shading according to CWIFT estimates (B) State bins map with shading according to CWIFT estimates](plots/state_heatmaps.png){#fig-state-heatmaps fig-alt="Two choropleth/heat maps created when examining state CWIFT estimates" fig-align="center"}

### Exploring County CWIFTs

The next natural step in the data exploration was to take a look at the county CWIFT estimates. While the state level CWIFTs provided a more macro view of teacher labor markets, we want to know if there is variation within state teacher labor markets and what that would look like.

The distribution of county CWIFTs was found to be similar to the distribution state CWIFTs. There is more variation, signaling there is significant within state variation, and a shift towards lower values. That is, there are more counties with lower CWIFT values. Let's skip looking at those parts of the data exploration and instead focus on within state variation in CWIFTs and how it compares across states.

As we can see in @fig-county-estimates, states like California and Maryland appear to have a high level variation in county CWIFTs while a state like South Dakota has very little variation in values. Once again it is difficult to understand potential spatial relationships so choropleth maps were constructed. 

![Grouped boxplot to examine county CWIFT estimates by state](plots/cwift_estimates_by_state.png){#fig-county-estimates fig-alt="Grouped boxplot to examine county CWIFT estimates by state" fig-align="center"}

@fig-us-county-heatmap seems to indicate that it is the west cost counties with the highest CWIFT. There are high values for east coast counties, but it seems to be contained to fewer counties. It is hard to see this given how small counties are in the east. We need to zoom in and look at states directly. Going from the US map to a single state requires only 1 line of code! Arranging state plots from different regions of the US takes 1 more line of code thanks to the `patchwork` package, 4-5 more lines to make it look nice. 

The end result is @fig-county-state-heatmap. California has high variance in county CWIFT values with most counties above 1; South Dakota has low variance in values with most below 1 for CWIFT; and Maryland has high variance in values with most values below 1 for CWIFT. 

![Choropleth/heat map to examine county CWIFT estimates](plots/county_cwift_heatmap.png){#fig-us-county-heatmap fig-alt="Choropleth/heat map to examine county CWIFT estimates" fig-align="center"}

![Demonstration of arranging plots to make comparisons between county CWIFT estimates within states. County maps with shading according to CWIFT estimates is provided for (A) California, (B) South Dakota, and (C) Maryland](plots/county_state_heatmaps.png){#fig-county-state-heatmap fig-alt="Choropleth/heat map to examine county CWIFT estimates" fig-align="center"}

### CWIFTs and Household Income

To explore this relationship we need to get state and county level estimates from the US Census Bureau. We could go to a website and download this information, but the `tidycensus` package makes getting this very easy. With 5 lines of code we can get this information and much more from the US Census Bureau. With a little data wrangling and a few lines of code to build the plots, we have the graphs in @fig-county-cwift-income. Yet more evidence of the power of the R ecosystem, since this can all be done in 10-15 lines of code. 

![Examining the relationship of CWIFT with median household income at (A) the state level and (B) the county level](plots/cwift_by_income_maps.png){#fig-county-cwift-income fig-alt="Examining the relationship of CWIFT with median household income at state and county levels." fig-align="center"}


## Comment on AI & R coding

Generative artificial intelligence, based on large language models (LLMs), looks to be a transformational assistive technology. It has the potential to supercharge productivity and is well on its way to becoming a standard tool in a researcher's tool kit. All facets of the research process are being impacted, which includes writing R code and conducting data analyses. There are several experimental packages or addins to leverage LLMs directly in RStudio: `{chattr}`, `{tidychatmodels}`, `{gptstudio}` and GitHub's Copilot to name a few. Copilot is explicitly for code-completion and generation. 

As far as coding and conducting data analyses, these tools have made it possible to simply provide prompts that contain basic guidance and in return we receive annotated code with explanations. They are capable of providing code exploratory data analyses. It is almost too good to be true, and it can be. Without sufficient knowledge of the R syntax and available tools like the tidyverse, it becomes difficult to judge the quality of the output or even if the output is worthwhile. So, for R-novices these tools can be a potential trap. Like all tools they need to be used wisely. A good starting point would be to use them as debuggers of your own code or asking them to improve upon your code. For more experienced R users, they can be used to generate starter/skeleton code. This can help cut down on some of the tedious coding work, but it requires a level of discernment that only comes with experience.

Before we jump right to using these tools in research, we need to be aware of data security and ethical concerns. We must be careful with the data these tools have access to. In fact, usage of LLMs may represent breaches of data use agreements and/or violation of privacy for study participants. We suggest a default practice of never exposing experimental data to AI tools, unless cleared in advanced. To be clear, AI tools can still be used to generate code, you just shouldn't include access to your data in the prompts.

Bottom line. AI tools can be very useful, epecially for coding. But they must be used with caution when handling confidential or private data.

## Conclusion

Hopefully this chapter has you convinced that learning R with RStudio and tidyverse would be a worthwhile investment of your time, especially for those with a focus on quantitative analyses. The R community is extremely supportive and there is a lot of material out there to help you get started. The **Suggested further reading** were selected to help you get started with these tools. Additionally, we hope you realize the importance of starting with best data practices to improve your research workflows, regardless of using R or not. 


------------------------------------------------------------------------

## Research essentials

This chapter discussed first steps of doing data work/research precisely because we feel it is not given enough time in traditional research curricula. We only touched on a few best practices, put there is so much more that students could add on to this R with RStudio and tidyverse setup. Two highly recommended must have additions are: 

- Learning how to use Quarto (.qmd files; <https://quarto.org/>) or R Markdown (.Rmd files) for authoring should be at the top of the list of tools to learn. Either can be used to author/create create paramaterized reports, books, websites, PhD thesis, research articles, etc. Consider exploring CiteDrive (<https://www.citedrive.com/en/>) to manage citations -- works very welll with Quarto. 

- Using version control, such as git, along with GitHub for collaboration. Having a trail of save points for your work, whether for coding or authoring, is extremely valuable when conducting research.

Fun fact, this chapter was written using these tools.

While it might seem like a lot of extra work and likely overkill for projects you have now, it is best to avoid developing bad habits. Try adding one thing at a time and challenge yourself to use these tools. Consider taking an old data project or past class work and convert it over to a workflow using these tools. Another option would be to replicate the work for the data exploration in this chapter (GitHub repo: <https://github.com/akuyper/cwift-examples>).

## Questions for further investigation 

How does getting setup compare to other statistical software you may have used? If setup was an issue, have you searched for any free workshops or materials online? Try using R with RStudio and the tidyverse by playing with a data project of your own and/or by downloading the data exploration examples RStudio project from its GitHub webpage (<https://github.com/akuyper/cwift-examples>). 

## Suggested further reading 

Hadley Wickham, Mine Çetinkaya-Rundel, & Grolemund, G. (2024). *R for data science (2e)*. <https://r4ds.hadley.nz/>. The is the starting point for almost anyone looking to learn R for data analyses. It is particularly good instilling best practices for data workflows. 

Hadley Wickham, Danielle Navarro, & Pedersen, T. L. (2024). *ggplot2: Elegant graphics for data analysis (3e)*. <https://ggplot2-book.org/>. Data visualization is such an important tool for data exploration and communication that this book is standard for anyone that wants to learn how to leverage everything `ggplot2` has to offer. 

*Tidyverse*. (2024). <https://www.tidyverse.org/>. The tidyverse website has en entire section dedicated to helping learners at any level and suggestions for other materials. 



## References

::: {#refs}
:::
